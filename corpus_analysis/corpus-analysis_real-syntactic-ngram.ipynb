{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9b99857",
   "metadata": {},
   "source": [
    "# Analyse syntaktischer N-Gramme  \n",
    "Basierend auf der Dependenzannotation mit `spaCy`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0874bcf",
   "metadata": {},
   "source": [
    "## Import der Bibliotheken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67c16814",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Dict, List, Union, Tuple\n",
    "from collections import OrderedDict, Counter\n",
    "from time import time\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import spacy\n",
    "from spacy.tokens import DocBin, Doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4236cbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a476052",
   "metadata": {},
   "source": [
    "Laden des spaCy-Modells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "! python -m spacy download de_core_news_sm\n",
    "nlp = spacy.load(\"de_core_news_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9970c60e",
   "metadata": {},
   "source": [
    "## Laden der Annotationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955f6b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bei Verwendung eines anderen Korpus hier den Verzeichnisnamen anpassen\n",
    "annotation_dir = Path(\"../data/spacy\")\n",
    "\n",
    "if not annotation_dir.exists():\n",
    "    print(\"The directory does not exist, please check the path again.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c98b489",
   "metadata": {
    "tags": [
     "skip-execution"
    ]
   },
   "outputs": [],
   "source": [
    "# Create dictionary to save the corpus data (filenames and tables)\n",
    "annotated_docs = {}\n",
    "\n",
    "start = time()\n",
    "# Iterate over spacy files\n",
    "for fp in tqdm(annotation_dir.iterdir(), desc=\"Reading annotated data\"):\n",
    "    # check if the entry is a file, not a directory\n",
    "    if fp.is_file():\n",
    "        # check if the file has the correct suffix spacy\n",
    "        if fp.suffix == '.spacy':\n",
    "            print( f\"Loading file: {fp.name}\" )\n",
    "            # load spacy DocBin objects\n",
    "            doc_bin = DocBin().from_disk(fp)\n",
    "            chunk_docs = list(doc_bin.get_docs(nlp.vocab))\n",
    "            # merge bins into one single document\n",
    "            full_doc = Doc.from_docs(chunk_docs)\n",
    "\n",
    "            # save the data frame to the dictionary, key=filename (without suffix), value=spacy.Doc\n",
    "            annotated_docs[fp.stem] = full_doc\n",
    "took = time() - start\n",
    "print(f\"Loading the data took: {round(took, 4)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fc9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Annotations of first 20 lines of the text: {list(annotated_docs.keys())[0]}:\\n\")\n",
    "print(\"Token\\tLemma\\tPoS\")\n",
    "for token in annotated_docs[list(annotated_docs.keys())[0]][:20]:\n",
    "    print(f\"{token.text}\\t{token.lemma_}\\t{token.pos_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb4a9945",
   "metadata": {},
   "source": [
    "## Metadaten einlesen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9071e6e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df = pd.read_csv(\"../metadata/metadata_corpus-german_language_fiction_1820-1900_50-per-decade.csv\")\n",
    "metadata_df = metadata_df[metadata_df['ID'].isin(annotated_docs.keys())]\n",
    "# Datentyp der Datumsspalte für eine einfachere Weiterverarbeitung ändern\n",
    "metadata_df['year'] = pd.to_datetime(metadata_df['year'], format=\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ce3bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df_alt = pd.read_csv(\"../metadata/metadata_corpus-german_language_fiction_1820-1900_50-per-decade_ALT.csv\")\n",
    "metadata_df_alt = metadata_df_alt[metadata_df_alt['ID'].isin(annotated_docs.keys())]\n",
    "# Datentyp der Datumsspalte für eine einfachere Weiterverarbeitung ändern\n",
    "metadata_df_alt['year'] = pd.to_datetime(metadata_df_alt['year'], format=\"%Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d6ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db01c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df_alt.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46652b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0811463d",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df['lastname'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9653ad17",
   "metadata": {},
   "source": [
    "## Syntaktische N-Gramme extrahieren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15873ce4",
   "metadata": {},
   "source": [
    "In einem weiteren Schritt können wir die Adjektive extrahieren, die mit dem Nomen Luft in Verbindung stehen. Wir machen dabei Gebrauch von den Dependenzstrukturen, die sich durch das spaCy-eigene `Doc` einfach navigieren lassen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d536a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dependent_adjective_list(spacy_docs: Dict, metadata_df: pd.DataFrame,\n",
    "                                     noun_input: Union[str, List[str]], top_n: int = 10) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"\n",
    "    Extract adjective modifiers (amod) for a noun or list of nouns and track their frequency over time.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    spacy_docs : dict\n",
    "        Dictionary with file_ids as keys and spaCy Doc objects as values\n",
    "    metadata_df : pd.DataFrame\n",
    "        DataFrame with columns: 'lastname', 'firstname', 'title', 'year', 'volume', 'ID', 'decade'\n",
    "    noun_input : str or list of str\n",
    "        Single noun lemma (e.g., 'liebe') or list of noun lemmata (e.g., ['liebe', 'leidenschaft'])\n",
    "    top_n : int\n",
    "        Number of most frequent adjectives to extract (default: 10)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    tuple : (pd.DataFrame, list)\n",
    "        - DataFrame with columns: filename, title, year, adjective, count, noun_count\n",
    "        - List of the top N adjectives found\n",
    "    \"\"\"\n",
    "    # Convert single noun to list for uniform processing\n",
    "    if isinstance(noun_input, str):\n",
    "        noun_list = [noun_input]\n",
    "    else:\n",
    "        noun_list = noun_input\n",
    "\n",
    "    # Convert to lowercase for case-insensitive matching\n",
    "    noun_list_lower = [noun.lower() for noun in noun_list]\n",
    "\n",
    "    # First pass: count all adjectives modifying any noun in the list across entire corpus\n",
    "    all_adjectives = Counter()\n",
    "\n",
    "    for file_id, doc in spacy_docs.items():\n",
    "        for token in doc:\n",
    "            # Check if this token is one of our target nouns\n",
    "            if token.lemma_.lower() in noun_list_lower and token.pos_ == 'NOUN':\n",
    "                # Find any dependent adjective\n",
    "                for child in token.children:\n",
    "                    #if child.dep_ == 'amod' and child.pos_ == 'ADJ':\n",
    "                    if child.pos_ == 'ADJ':\n",
    "                        all_adjectives[child.lemma_.lower()] += 1\n",
    "\n",
    "    # Get top N most frequent adjectives\n",
    "    top_adjectives = [adj for adj, count in all_adjectives.most_common(top_n)]\n",
    "\n",
    "    # Second pass: calculate frequencies per document\n",
    "    results = []\n",
    "\n",
    "    for file_id, doc in spacy_docs.items():\n",
    "        # Get metadata for this file\n",
    "        meta_row = metadata_df[metadata_df['ID'] == file_id]\n",
    "\n",
    "        if meta_row.empty:\n",
    "            continue\n",
    "\n",
    "        # Count adjectives modifying the target nouns\n",
    "        adjective_counts = Counter()\n",
    "        noun_count = 0\n",
    "\n",
    "        for token in doc:\n",
    "            # Check if this token is one of our target nouns\n",
    "            if token.lemma_.lower() in noun_list_lower and token.pos_ == 'NOUN':\n",
    "                noun_count += 1\n",
    "                #print('found noun:', token.text)\n",
    "                # Find adjective modifiers (amod dependency)\n",
    "                for child in token.children:\n",
    "                    #print('child dep:', child.dep_, 'pos:', child.pos_)\n",
    "                    #if child.dep_ == 'amod' and child.pos_ == 'ADJ':\n",
    "                    if child.pos_ == 'ADJ':\n",
    "                        #print('  found dependent adjective:', child.text)\n",
    "                        adjective_counts[child.lemma_.lower()] += 1\n",
    "\n",
    "        # Create a row for each top adjective found in this document\n",
    "        # (even if count is 0, we want to track that)\n",
    "        for adjective in top_adjectives:\n",
    "            count = adjective_counts.get(adjective, 0)\n",
    "            if noun_count > 0 or count > 0:  # Include if we have nouns or this adjective\n",
    "                results.append({\n",
    "                    'filename': file_id,\n",
    "                    'title': meta_row['title'].values[0],\n",
    "                    'year': meta_row['year'].values[0],\n",
    "                    'adjective': adjective,\n",
    "                    'count': count,\n",
    "                    'noun_count': noun_count\n",
    "                })\n",
    "\n",
    "    return pd.DataFrame(results), top_adjectives\n",
    "\n",
    "\n",
    "def get_top_adjectives_list(adj_df: pd.DataFrame, top_n: int = 10) -> list:\n",
    "    \"\"\"\n",
    "    Get the top N most frequent adjectives from the adjective dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    adj_df : pd.DataFrame\n",
    "        DataFrame returned by extract_adjective_modifiers_list()\n",
    "    top_n : int\n",
    "        Number of most frequent adjectives to return\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    list\n",
    "        List of top N adjectives\n",
    "    \"\"\"\n",
    "    total_counts = adj_df.groupby('adjective')['count'].sum().sort_values(ascending=False)\n",
    "    return total_counts.head(top_n).index.tolist()\n",
    "\n",
    "\n",
    "def plot_adjective_trends_list(adj_df: pd.DataFrame, top_adjectives: list,\n",
    "                               noun_input: Union[str, List[str]]):\n",
    "    \"\"\"\n",
    "    Create a plot showing adjective modifier trends over time for a noun or noun list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    adj_df : pd.DataFrame\n",
    "        DataFrame returned by extract_adjective_modifiers_list()\n",
    "    top_adjectives : list\n",
    "        List of adjectives to plot (e.g., from get_top_adjectives_list())\n",
    "    noun_input : str or list of str\n",
    "        The noun(s) being analyzed\n",
    "    show_individual_texts : bool\n",
    "        If True, show individual text data points with titles; if False, show yearly means only\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    plotly.graph_objects.Figure\n",
    "        The figure object (will display automatically in Jupyter)\n",
    "    \"\"\"\n",
    "    # Filter for only the top adjectives\n",
    "    filtered_df = adj_df[adj_df['adjective'].isin(top_adjectives)].copy()\n",
    "\n",
    "    # Calculate relative frequency (per 100 noun occurrences)\n",
    "    # Handle division by zero\n",
    "    filtered_df['rel_freq'] = filtered_df.apply(\n",
    "        lambda row: (row['count'] / row['noun_count']) * 100 if row['noun_count'] > 0 else 0,\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "\n",
    "    # Show individual texts as scatter points\n",
    "    for adj in top_adjectives:\n",
    "        adj_data = filtered_df[filtered_df['adjective'] == adj]\n",
    "\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=adj_data['year'],\n",
    "            y=adj_data['rel_freq'],\n",
    "            mode='markers',\n",
    "            name=adj,\n",
    "            text=adj_data['title'],\n",
    "            hovertemplate='<b>%{fullData.name}</b><br>' +\n",
    "                         '<b>%{text}</b><br>' +\n",
    "                         'Year: %{x}<br>' +\n",
    "                         'Frequency: %{y:.2f} per 100 occurrences<br>' +\n",
    "                         '<extra></extra>',\n",
    "            marker=dict(size=8, opacity=0.7)\n",
    "        ))\n",
    "\n",
    "    # Create title based on input\n",
    "    if isinstance(noun_input, str):\n",
    "        title_noun = f'\"{noun_input}\"'\n",
    "    else:\n",
    "        noun_str = ', '.join(noun_input[:3])\n",
    "        if len(noun_input) > 3:\n",
    "            noun_str += f', ... ({len(noun_input)} total)'\n",
    "        title_noun = f'[{noun_str}]'\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'Adjective Syntactic Dependents of {title_noun} Over Time',\n",
    "        xaxis_title='Year',\n",
    "        yaxis_title=f'Relative Frequency (per 100 noun occurrences)',\n",
    "        hovermode='closest',\n",
    "        height=600,\n",
    "        legend=dict(\n",
    "            title='Adjectives',\n",
    "            yanchor=\"top\",\n",
    "            y=0.99,\n",
    "            xanchor=\"right\",\n",
    "            x=0.99\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "def plot_adjective_trends_moving_avg(\n",
    "    adj_df: pd.DataFrame,\n",
    "    top_adjectives: list,\n",
    "    noun_input: Union[str, List[str]],\n",
    "    window_years: int = 10,\n",
    "    n_plot: int = 8,\n",
    "    value_col: str = \"rel_freq\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Lineplot per year for adjective dependents + centered moving average window.\n",
    "\n",
    "    Expects adj_df to have at least: year, adjective, count, noun_count\n",
    "    If rel_freq is missing, it will be computed as (count / noun_count) * 100.\n",
    "    \"\"\"\n",
    "\n",
    "    df = adj_df.copy()\n",
    "\n",
    "    # Ensure year is numeric (sometimes comes in as str)\n",
    "    # If year is datetime-like, convert to integer year\n",
    "    if pd.api.types.is_datetime64_any_dtype(df[\"year\"]):\n",
    "        df[\"year\"] = df[\"year\"].dt.year\n",
    "    else:\n",
    "        df[\"year\"] = pd.to_numeric(df[\"year\"], errors=\"coerce\")\n",
    "    df = df.dropna(subset=[\"year\"])\n",
    "    df[\"year\"] = df[\"year\"].astype(int)\n",
    "\n",
    "    # Compute relative frequency if needed\n",
    "    if value_col not in df.columns:\n",
    "        df[value_col] = df.apply(\n",
    "            lambda row: (row[\"count\"] / row[\"noun_count\"]) * 100 if row[\"noun_count\"] > 0 else 0,\n",
    "            axis=1\n",
    "        )\n",
    "\n",
    "    # Keep only selected adjectives\n",
    "    df = df[df[\"adjective\"].isin(top_adjectives)].copy()\n",
    "\n",
    "    # Yearly aggregate (mean across texts for each year)\n",
    "    yearly = (\n",
    "        df.groupby([\"year\", \"adjective\"])[value_col]\n",
    "          .mean()\n",
    "          .unstack(\"adjective\")\n",
    "          .sort_index()\n",
    "    )\n",
    "\n",
    "    # Moving average\n",
    "    moving = yearly.rolling(window=window_years, center=True, min_periods=1).mean()\n",
    "\n",
    "    # Title\n",
    "    if isinstance(noun_input, str):\n",
    "        title_noun = f'\"{noun_input}\"'\n",
    "    else:\n",
    "        noun_str = \", \".join(noun_input[:3])\n",
    "        if len(noun_input) > 3:\n",
    "            noun_str += f\", ... ({len(noun_input)} total)\"\n",
    "        title_noun = f\"[{noun_str}]\"\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(14, 7))\n",
    "    for adj in list(moving.columns)[:n_plot]:\n",
    "        plt.plot(moving.index, moving[adj], linewidth=2.5, marker=\"o\", markersize=4, label=adj)\n",
    "\n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(f\"{value_col} (moving avg, window={window_years}y)\")\n",
    "    plt.title(f\"Top {min(n_plot, len(top_adjectives))} Adjectives – {window_years}-Year Moving Average – {title_noun}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return yearly, moving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaf53ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "noun = \"Luft\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecc5e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df, top_adjs = extract_dependent_adjective_list(annotated_docs, metadata_df, noun, top_n=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d1e16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_df_alt, top_adjs_alt = extract_dependent_adjective_list(annotated_docs, metadata_df_alt, noun, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a35d9c",
   "metadata": {},
   "source": [
    "## Analyse und Visualisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317497d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 1\n",
    "plot_adjective_trends_list(adj_df, top_adjs, noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dfa785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: yearly lineplots + moving average\n",
    "yearly_1, moving_1 = plot_adjective_trends_moving_avg(\n",
    "    adj_df, top_adjs, noun_input=noun, window_years=10, n_plot=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3835fc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 2\n",
    "plot_adjective_trends_list(adj_df_alt, top_adjs_alt, noun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1906e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEW: yearly lineplots + moving average\n",
    "yearly_2, moving_2 = plot_adjective_trends_moving_avg(\n",
    "    adj_df_alt, top_adjs_alt, noun_input=noun, window_years=10, n_plot=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b1d19a",
   "metadata": {},
   "source": [
    "## Add plots for specific words that we found"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2065f48c",
   "metadata": {},
   "source": [
    "TO BE CHANGED / UPDATED / CONTINUED"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
